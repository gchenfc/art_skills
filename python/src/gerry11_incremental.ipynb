{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm.notebook as tqdm\n",
    "import gtsam\n",
    "from gtsam.symbol_shorthand import X, P\n",
    "\n",
    "import loader\n",
    "import sln_fit\n",
    "from fit_types import OptimizationLoggingParams\n",
    "import utils\n",
    "from utils import Point2_, Double_\n",
    "from art_skills import SlnStrokeExpression2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_data = loader.load_segments('A', index=None)\n",
    "print(f'total letter duration is approx {sum(traj[-1][-1, 0] for traj in letter_data):.5f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISAM2 with 1 stroke\n",
    "\n",
    "Note: this doesn't actually really take advantage of ISAM at all, since there's only 1 variable.\n",
    "\n",
    "But at least we can learn the mechanics of ISAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an incremental solve on just 1 stroke\n",
    "# Configure\n",
    "isam2params = gtsam.ISAM2Params()\n",
    "isam2params.evaluateNonlinearError = True\n",
    "isam2params.relinearizeSkip = 1\n",
    "fit_params = sln_fit.FitStrokeParams()\n",
    "logging_params = OptimizationLoggingParams(print_progress=False)\n",
    "strokedata = iter(letter_data[0][0])\n",
    "# Initialize\n",
    "isam = gtsam.ISAM2(isam2params)\n",
    "strokei = 0\n",
    "k = 0\n",
    "history = []\n",
    "stroke = SlnStrokeExpression2(P(strokei))\n",
    "\n",
    "# Run\n",
    "with utils.Time('Incremental Solve') as t:\n",
    "    # stroke needs to start with some amount of data\n",
    "    graph = gtsam.NonlinearFactorGraph()\n",
    "    for datai in range(10):\n",
    "        graph.add(sln_fit.create_factor(*next(strokedata), stroke, fit_params, strokei))\n",
    "        k += 1\n",
    "    init = gtsam.Values()\n",
    "    init.insert(P(strokei), np.array([letter_data[0][strokei][0, 0] - 0.05, 1., 0., 0., 0.5, -0.5]))\n",
    "    init.insert(X(strokei), letter_data[0][strokei][0, 1:])\n",
    "    init, _ = utils.solve(graph, init, fit_params.lm_params, logging_params)\n",
    "    history.append((isam.update(graph, init), k,\n",
    "                    sln_fit.StrokeUtils.values2solution(isam.calculateBestEstimate(), stroke,\n",
    "                                                        letter_data[0][strokei], strokei)))\n",
    "    print(f'Initialization (t <= {letter_data[0][strokei][datai, 0]}) took {t()} seconds')\n",
    "    # Now do incremental\n",
    "    for datapoint in strokedata:\n",
    "        k += 1\n",
    "        graph = gtsam.NonlinearFactorGraph()\n",
    "        graph.add(sln_fit.create_factor(*datapoint, stroke, fit_params, strokei))\n",
    "        isam.update(graph, gtsam.Values())\n",
    "        for _ in range(3):\n",
    "            isam.update()\n",
    "        history.append((isam.update(), k,\n",
    "                        sln_fit.StrokeUtils.values2solution(isam.calculateBestEstimate(), stroke,\n",
    "                                                            letter_data[0][strokei], strokei)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "rows = int(np.sqrt(len(history)))\n",
    "cols = len(history) // rows + 1\n",
    "fig, axes = plt.subplots(rows, cols, figsize = (12, 12))\n",
    "axes = axes.flatten()\n",
    "for (_, k, est), ax in zip(history, axes):\n",
    "    ax.plot(est['data'][:, 1], est['data'][:, 2], 'k.')\n",
    "    ax.plot(est['txy'][:k, 1], est['txy'][:k, 2], 'r.')\n",
    "    ax.plot(est['txy'][k:, 1], est['txy'][k:, 2], 'mx')\n",
    "    ax.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to handle multiple strokes\n",
    "\n",
    "Initially I tried just doing the way we have been doing, e.g.:\n",
    "```\n",
    "data[0] = x0 + eval(p0, t = 0)\n",
    "data[1] = x0 + eval(p0, t = 1)\n",
    "data[2] = x0 + eval(p0, t = 2)\n",
    "data[3] = x1 + eval(p1, t = 3)\n",
    "data[4] = x1 + eval(p1, t = 4)\n",
    "data[5] = x1 + eval(p1, t = 5)\n",
    "data[6] = x2 + eval(p2, t = 6)\n",
    "data[7] = x2 + eval(p2, t = 7)\n",
    "data[8] = x2 + eval(p2, t = 8)\n",
    "```\n",
    "\n",
    "But this was causing issues.  Specifically, the strokes would get centered (t0) at really far away times so that the actual used part of the stroke was very sensitive to numerical difficulties and poorly conditioned.  (Note: I also might have had a bug in initializing t0, but since I deleted the multi-stroke code, I can't remember anymore.)\n",
    "\n",
    "So I realized that probably we need to add the strokes properly.  This will help since, if e.g. stroke1 tries to start earlier, then it will mess up stroke0.  So if we add strokes properly, then the strokes should naturally avoid these types of conditioning problems.\n",
    "\n",
    "I think there are 4 natural steps to take:\n",
    "1. Add every stroke for every data point, and use full re-solving (with warm-starts)\n",
    "2. Add every stroke for every data point, and use marginalization to get rid of old strokes\n",
    "3. Discard when stroke contributions are very small (only keep the small overlaps), and full re-solving (with warm-starts)\n",
    "4. Discard when stroke contributions are very small (only keep the small overlaps), and use ISAM2.\n",
    "\n",
    "To clarify, 1 and 2 are like:\n",
    "```\n",
    "data[0] = x0 + eval(p0, t = 0) + eval(p1, t = 0) + eval(p2, t = 0)\n",
    "data[1] = x0 + eval(p0, t = 1) + eval(p1, t = 1) + eval(p2, t = 1)\n",
    "data[2] = x0 + eval(p0, t = 2) + eval(p1, t = 2) + eval(p2, t = 2)\n",
    "data[3] = x0 + eval(p0, t = 3) + eval(p1, t = 3) + eval(p2, t = 3)\n",
    "data[4] = x0 + eval(p0, t = 4) + eval(p1, t = 4) + eval(p2, t = 4)\n",
    "data[5] = x0 + eval(p0, t = 5) + eval(p1, t = 5) + eval(p2, t = 5)\n",
    "...\n",
    "```\n",
    "\n",
    "Whereas 3 and 4 are like:\n",
    "...TODO...\n",
    "The scheme we will use is to keep a list of all the strokes, but when a stroke gets \"small\" (the contribution is small), then we will \"freeze\" the contributions.  i.e.\n",
    "data[0] = x0 + eval(p0, t = 0)\n",
    "data[1] = x0 + eval(p0, t = 1)\n",
    "data[2] = x0 + eval(p0, t = 2)\n",
    "data[3] = x0 + eval(p0, t = 3)\n",
    "data[4] = x0 + eval(p0, t = 4) + eval(p1, t = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data[0] = x0 + eval(p0, t = 0)\n",
    "data[1] = x0 + eval(p0, t = 1)\n",
    "data[2] = x0 + eval(p0, t = 2)\n",
    "data[3] = x0 + eval(p0, t = 3)\n",
    "data[3] = x0 + eval(p0, t = 3)\n",
    "data[3] = x0 + eval(p0, t = 3)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The scheme we will use is to keep a list of all the strokes, but when a stroke gets \"small\" (the contribution is small), then we will \"freeze\" the contributions.  i.e.\n",
    "data[0] = x0 + eval(p0, t = 0)\n",
    "data[1] = x0 + eval(p0, t = 1)\n",
    "data[2] = x0 + eval(p0, t = 2)\n",
    "data[3] = x0 + eval(p0, t = 3)\n",
    "data[4] = x0 + eval(p0, t = 4) + eval(p1, t = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Every stroke contributes to every point: Full Re-solving using warm-starts (No ISAM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util\n",
    "def create_graph(data, strokes):\n",
    "    graph = gtsam.NonlinearFactorGraph()\n",
    "    for t, x, y in data:\n",
    "        graph.add(sln_fit.create_factor2(t, x, y, strokes, fit_params))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "traj_data = np.concatenate([stroke for stroke in letter_data[0]])\n",
    "inds_to_add_stroke = np.array([0] + np.cumsum([stroke.shape[0]\n",
    "                                               for stroke in letter_data[0]]).tolist())\n",
    "strokes = []\n",
    "strokei = 0\n",
    "history = []\n",
    "init = gtsam.Values()\n",
    "init.insert(X(strokei), traj_data[0, 1:])\n",
    "fit_params.lm_params.setRelativeErrorTol(1e-5)\n",
    "fit_params.lm_params.setMaxIterations(3)\n",
    "\n",
    "# Run\n",
    "with utils.Time('Incremental Solve') as t:\n",
    "    # for k in tqdm.trange(100):\n",
    "    for k in tqdm.trange(traj_data.shape[0]):\n",
    "        if k == inds_to_add_stroke[strokei]:\n",
    "            strokes.append(SlnStrokeExpression2(P(strokei)))\n",
    "            if strokei == 0:\n",
    "                init.insert(P(strokei), np.array([traj_data[k, 0] - 0.05, 1., 0., 0., 0.5, -0.5]))\n",
    "            else:\n",
    "                p = init.atVector(P(strokei - 1))\n",
    "                init.insert(P(strokei), np.array([traj_data[k, 0] - 0.05, 1., p[3], p[3], 0.5, -0.5]))\n",
    "            strokei += 1\n",
    "        if k < 10:\n",
    "            continue\n",
    "        # if k % 3 < 2:\n",
    "        #     continue\n",
    "        init, _ = utils.solve(create_graph(traj_data[:k], strokes), init, fit_params.lm_params,\n",
    "                            logging_params)\n",
    "        # init, _ = utils.solve(create_graph(traj_data[:k], strokes), init, fit_params.lm_params)\n",
    "        history.append((k, gtsam.Values(init), t()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [0] + [t for _, _, t in history]\n",
    "plt.plot([k for k, _, _ in history], np.diff(times), '.')\n",
    "print(inds_to_add_stroke)\n",
    "for ind in inds_to_add_stroke:\n",
    "    plt.axvline(ind, color='r', label='Another Stroke' if ind == 0 else None)\n",
    "plt.title('Solve times with various numbers of strokes (warm-start)')\n",
    "plt.xlabel('Data frame index number')\n",
    "plt.ylabel('Solve time (seconds)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util to convert Values to txy\n",
    "ts = traj_data[:, 0]\n",
    "xy = lambda t, est, strokes: sum((stroke.pos(t, True, np.zeros(2), est) for stroke in strokes),\n",
    "                        Point2_(X(0)).value(est))\n",
    "speed = lambda t, est, strokes: sum(stroke.speed(t, True, est) for stroke in strokes)\n",
    "xys = []\n",
    "speeds = []\n",
    "for k, est, _ in history[::11]:\n",
    "    # xys.append(np.array([xy(t, est, strokes[:sum(k > inds_to_add_stroke)]) for t in ts[:k]]))\n",
    "    xys.append((k, np.array([xy(t, est, strokes[:sum(k > inds_to_add_stroke)]) for t in ts])))\n",
    "    # speeds.append(np.array([speed(t, est, strokes[:sum(k > inds_to_add_stroke)]) for t in ts[:k]]))\n",
    "    speeds.append(np.linalg.norm(np.diff(xys[-1][1][:k, :], axis=0), axis=1) * 120)\n",
    "speed_gt = np.linalg.norm(np.diff(traj_data[:, 1:], axis=0), axis=1) * 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot xy\n",
    "fig, axes = plt.subplots(len(xys) // 10 + 1, 10, figsize=(16, (len(xys) // 10 + 1) * 1.6))\n",
    "for ax, (k, xy) in zip(axes.flatten(), xys):\n",
    "    ax.plot(traj_data[:, 1], traj_data[:, 2], 'k.')\n",
    "    ax.plot(xy[k:k+12, 0], xy[k:k+12, 1], 'rx')\n",
    "    ax.plot(xy[:k, 0], xy[:k, 1], 'g.')\n",
    "    ax.axis('equal')\n",
    "plt.suptitle('Incremental Solve (warm-start)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot speed\n",
    "fig, axes = plt.subplots(len(xys) // 10 + 1, 10, figsize=(16, (len(xys) // 10 + 1) * 1.6), sharex=True, sharey=True)\n",
    "for ax, speed in zip(axes.flatten(), speeds):\n",
    "    ax.plot(speed_gt, 'k')\n",
    "    ax.plot(speed, 'r')\n",
    "for (k, est, _), ax in zip(history[::11], axes.flatten()):\n",
    "    strokes_ = strokes[:sum(k > inds_to_add_stroke)]\n",
    "    ax.plot(np.array([[stroke.speed(t, True, est) for stroke in strokes_] for t in ts]), '--')\n",
    "axes[0, 0].set_ylim(0, 5)\n",
    "plt.suptitle('Incremental Solve (warm-start): Speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Every stroke contributes to every point: Marginalization (No ISAM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "traj_data = np.concatenate([stroke for stroke in letter_data[0]])\n",
    "inds_to_add_stroke = np.array([0] +\n",
    "                              np.cumsum([stroke.shape[0] for stroke in letter_data[0]]).tolist())\n",
    "strokes = []\n",
    "strokei = 0\n",
    "history = []\n",
    "init = gtsam.Values()\n",
    "init.insert(X(strokei), traj_data[0, 1:])\n",
    "fit_params.lm_params.setRelativeErrorTol(1e-5)\n",
    "fit_params.lm_params.setMaxIterations(3)\n",
    "\n",
    "# Run\n",
    "with utils.Time('Incremental Solve') as t:\n",
    "    # for k in tqdm.trange(100):\n",
    "    for k in tqdm.trange(traj_data.shape[0]):\n",
    "        if k == inds_to_add_stroke[strokei]:\n",
    "            strokes.append(SlnStrokeExpression2(P(strokei)))\n",
    "            if strokei == 0:\n",
    "                init.insert(P(strokei), np.array([traj_data[k, 0] - 0.05, 1., 0., 0., 0.5, -0.5]))\n",
    "            else:\n",
    "                p = init.atVector(P(strokei - 1))\n",
    "                init.insert(P(strokei),\n",
    "                            np.array([traj_data[k, 0] - 0.05, 1., p[3], p[3], 0.5, -0.5]))\n",
    "            if strokei > 1:\n",
    "                p = init.atVector(P(strokei - 2))\n",
    "                strokes[strokei - 2] = SlnStrokeExpression2(p)\n",
    "                init.erase(P(strokei - 2))\n",
    "            strokei += 1\n",
    "        if k < 10:\n",
    "            continue\n",
    "        fit_params.lm_params.setMaxIterations(100)\n",
    "        if k > 10:\n",
    "            fit_params.lm_params.setMaxIterations(3)\n",
    "        # if k % 3 < 2:\n",
    "        #     continue\n",
    "        init, _ = utils.solve(create_graph(traj_data[:k], strokes), init, fit_params.lm_params,\n",
    "                              logging_params)\n",
    "        # init, _ = utils.solve(create_graph(traj_data[:k], strokes), init, fit_params.lm_params)\n",
    "        history.append((k, gtsam.Values(init), t()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [0] + [t for _, _, t in history]\n",
    "plt.plot([k for k, _, _ in history], np.diff(times), '.')\n",
    "print(inds_to_add_stroke)\n",
    "for ind in inds_to_add_stroke:\n",
    "    plt.axvline(ind, color='r', label='Another Stroke' if ind == 0 else None)\n",
    "plt.title('Solve times with various numbers of strokes (warm-start)')\n",
    "plt.xlabel('Data frame index number')\n",
    "plt.ylabel('Solve time (seconds)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util to convert Values to txy\n",
    "ts = traj_data[:, 0]\n",
    "xy = lambda t, est, strokes: sum((stroke.pos(t, True, np.zeros(2), est) for stroke in strokes),\n",
    "                        Point2_(X(0)).value(est))\n",
    "speed = lambda t, est, strokes: sum(stroke.speed(t, True, est) for stroke in strokes)\n",
    "xys = []\n",
    "speeds = []\n",
    "for k, est, _ in history[::11]:\n",
    "    # xys.append(np.array([xy(t, est, strokes[:sum(k > inds_to_add_stroke)]) for t in ts[:k]]))\n",
    "    xys.append((k, np.array([xy(t, est, strokes[:sum(k > inds_to_add_stroke)]) for t in ts])))\n",
    "    # speeds.append(np.array([speed(t, est, strokes[:sum(k > inds_to_add_stroke)]) for t in ts[:k]]))\n",
    "    speeds.append(np.linalg.norm(np.diff(xys[-1][1][:k, :], axis=0), axis=1) * 120)\n",
    "speed_gt = np.linalg.norm(np.diff(traj_data[:, 1:], axis=0), axis=1) * 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot xy\n",
    "fig, axes = plt.subplots(len(xys) // 10 + 1, 10, figsize=(16, (len(xys) // 10 + 1) * 1.6))\n",
    "for ax, (k, xy) in zip(axes.flatten(), xys):\n",
    "    ax.plot(traj_data[:, 1], traj_data[:, 2], 'k.')\n",
    "    ax.plot(xy[k:k+12, 0], xy[k:k+12, 1], 'rx')\n",
    "    ax.plot(xy[:k, 0], xy[:k, 1], 'g.')\n",
    "    ax.axis('equal')\n",
    "plt.suptitle('Incremental Solve (warm-start)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot speed\n",
    "fig, axes = plt.subplots(len(xys) // 10 + 1, 10, figsize=(16, (len(xys) // 10 + 1) * 1.6), sharex=True, sharey=True)\n",
    "for ax, speed in zip(axes.flatten(), speeds):\n",
    "    ax.plot(speed_gt, 'k')\n",
    "    ax.plot(speed, 'r')\n",
    "for (k, est, _), ax in zip(history[::11], axes.flatten()):\n",
    "    strokes_ = strokes[:sum(k > inds_to_add_stroke)]\n",
    "    ax.plot(np.array([[stroke.speed(t, True, est) for stroke in strokes_] for t in ts]), '--')\n",
    "axes[0, 0].set_ylim(0, 5)\n",
    "plt.suptitle('Incremental Solve (warm-start): Speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "402f513bd64bb05ccdfd11315d0c88453571d1d1d73db48414a1b2a41f771ebc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
